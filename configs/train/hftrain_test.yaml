# @package _global_
# HuggingFace Trainer Configuration for Local Testing

# Optimizer settings (reduced for testing)
optimizer: adamw
lr: 5e-5
weight_decay: 0.01

# Training schedule (reduced for testing)
epochs: 2
batch: 2
num_workers: 2

# HuggingFace Trainer specific settings
hf_trainer:
  # Core training arguments
  output_dir: ./outputs/hf_training_test
  overwrite_output_dir: true
  
  # Training parameters (reduced for testing)
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 1
  
  # Learning rate and scheduling
  learning_rate: 5e-5
  weight_decay: 0.01
  warmup_steps: 10
  lr_scheduler_type: "linear"
  
  # Training duration (reduced for testing)
  num_train_epochs: 2
  max_steps: 50  # Override epochs for quick testing
  
  # Optimization
  fp16: false
  bf16: false  # Disable for testing
  tf32: true
  gradient_checkpointing: false  # Disable for faster testing
  
  # Gradient clipping
  max_grad_norm: 1.0
  
  # Evaluation
  evaluation_strategy: "steps"
  eval_steps: 10
  eval_accumulation_steps: 1
  
  # Logging (more frequent for testing)
  logging_strategy: "steps"
  logging_steps: 5
  logging_first_step: true
  
  # Saving (more frequent for testing)
  save_strategy: "steps"
  save_steps: 20
  save_total_limit: 2
  load_best_model_at_end: false  # Disable for testing
  
  # Reproducibility
  seed: 42
  data_seed: 42
  
  # Data loading (reduced for testing)
  dataloader_num_workers: 2
  dataloader_pin_memory: true
  dataloader_persistent_workers: false
  
  # Distributed training
  ddp_find_unused_parameters: true
  ddp_bucket_cap_mb: 25
  ddp_broadcast_buffers: false
  
  # Memory optimization
  dataloader_drop_last: true
  remove_unused_columns: false
  
  # Reporting
  report_to: []  # Disable reporting for testing
  logging_dir: "./outputs/hf_training_test/logs"
  
  # Advanced settings
  push_to_hub: false
  resume_from_checkpoint: null
  ignore_data_skip: false
  
  # Model saving format
  save_safetensors: true