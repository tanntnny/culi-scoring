optimizer: adamw
lr: 1e-4
weight_decay: 0.01
epochs: 20
batch: 4
num_workers: 8

# Enable Automatic Mixed Precision training
amp: true

# Gradient accumulation steps (1 = no accumulation)
grad_accum: 1

# Flag passed to DistributedDataParallel to handle modules with conditional branches
ddp_find_unused_parameters: true

# Set static graph mode for DDP to avoid re-registering hooks (recommended when using checkpointing)
ddp_static_graph: true

# Log training metrics every N steps (matches trainer expectation)
log_every_n: 10

# Gradient clipping threshold
clip_grad: 1.0

# Profiler
profiler:
  enabled: true
  activities: [cpu, cuda]
  record_shapes: false
  profile_memory: false
  with_stack: false
  topk: 10
  tensorboard: true
  rank_zero_only: true
  schedule:
    wait: 1
    warmup: 1
    active: 4
    repeat: 1
    skip_first: 0